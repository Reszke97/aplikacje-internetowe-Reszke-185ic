# Lab 5 â€“ WebScrapping

## Å¹rÃ³dÅ‚o
ğŸ”— https://zacniewski.gitlab.io/teaching/2020-internet-apps/

# Aby wykonaÄ‡ laboratorium stworzyÅ‚em nowÄ… aplikacjÄ™. DodaÅ‚em do niej 3 widoki:
- Widok **home** jako stronÄ™ gÅ‚Ã³wnÄ… a na niej wykonane przykÅ‚ady z wykÅ‚adu,
- Widok **scraping** a w nim formularz, w ktÃ³rym moÅ¼emy podaÄ‡ stronÄ™ i szukany element do scrapowania
- Widok **xpath** a do tego widoku dodane zostaÅ‚y elementy z innych stron za pomocÄ… xPath i lxmlx

## Przed rozpoczÄ™ciem dodawania elementÃ³w naleÅ¼aÅ‚o pobraÄ‡ pakiet **Beautifulsoup4** za pomocÄ… polecenia ``pip install beautifulsoup4`` oraz pakiet **lxml** za pomocÄ… polecenia ``pip install lxml``.

## Tak przedstawia siÄ™ widok home:
![](https://github.com/Reszke97/aplikacje-internetowe-Reszke-185ic/blob/master/lab5/zrzuty/1.PNG)

### Do wyÅ›wietlenia przykÅ‚adÃ³w z repozytorium wystarczyÅ‚o przekazaÄ‡ do funkcji return funkcje **render** ze sÅ‚ownikiem, z szablonem do ktÃ³rego bÄ™dziemy przekazywaÄ‡ dane oraz sÅ‚ownikiem z utowrzonymi danymi z web scrapingu.
Tak wyglÄ…da kod z przykÅ‚adami z repozytorium:

```python
page = requests.get("https://codedamn-classrooms.github.io/webscraper-python-codedamn-classroom-website/")
    soup = BeautifulSoup(page.content, "html.parser")
    # PrzykÅ‚ad 1
    # Create all_h1_tags as empty list
    all_h1_tags = []

    # Set all_h1_tags to all h1 tags of the soup
    for element in soup.select("h1"):
        all_h1_tags.append(element.text)

    # Create seventh_p_text and set it to 7th p element text of the page
    seventh_p_text = soup.select("p")[6].text


    # PrzykÅ‚ad 2
    # Create top_items as empty list
    top_items = []

    # Extract and store in top_items according to instructions on the left
    products = soup.select("div.thumbnail")
    for elem in products:
        title = elem.select("h4 > a.title")[0].text
        review_label = elem.select("div.ratings")[0].text
        # strip() usuwa zbÄ™dne spacje
        info = {"title": title.strip(), "review": review_label.strip()}
        top_items.append(info)
    

    #PrzykÅ‚ad 3
    
    # ZmodyfikowaÅ‚em przykÅ‚ad 3 za pomocÄ… wyraÅ¼enia regularnego
    # Create top_items as empty list
    all_links = []

    # Extract and store in top_items according to instructions on the left
    # znajdÅº wszystkie tagi a ktÃ³rch atrubut href zaczyna siÄ™ od "https"
    links = soup.find_all('a', {'href':re.compile('^https')})
    for ahref in links:
        text = ahref.text
        text = text.strip() if text is not None else ""

        
        href = ahref.get("href")
        href = href.strip() if href is not None else ""
        all_links.append({"href": href, "text": text})

    # PrzykÅ‚ad 4
    all_products = []

    # Extract and store in top_items according to instructions on the left
    products = soup.select('div.thumbnail')
    for product in products:
        name = product.select('h4 > a')[0].text.strip()
        description = product.select('p.description')[0].text.strip()
        price = product.select('h4.price')[0].text.strip()
        reviews = product.select('div.ratings')[0].text.strip()
        image = product.select('img')[0].get('src')

        all_products.append({
            "name": name,
            "description": description,
            "price": price,
            "reviews": reviews,
            "image": image
        })  
    return render(request,'scrapping/home.html',{'top_items':top_items,'all_h1_tags':all_h1_tags, 
        'seventh_p_text':seventh_p_text,'all_links':all_links,'all_products':all_products})
```
## W przykÅ‚adzie nr 3 zmodyfikowaÅ‚em kod w taki sposÃ³b aby dziÄ™ki wyraÅ¼eniom regualarnym otrzymaÄ‡ wszystkie linki zaczynajÄ…ce siÄ™ od "https" a nastÄ™pnie w szablonie dodaÅ‚em je do tagu ```a``` do atrubutu ```href```
## Tak przedstawia siÄ™ div z przykÅ‚adu nr 3 w szablonie home:


```python
<div class="twoRemPaddig">
    <h1>PrzykÅ‚ad 3</h1>
    {% for links in all_links %}
        <a href = "{{links.href}}" >{{links.text}}</a>   
    {% endfor %}
</div>
```

## Do stworzenia formularza wykorzystaÅ‚em gotowy formularz z bootstrapa https://getbootstrap.com/docs/4.0/components/forms/ . DostosowaÅ‚em go do potrzeb laboratoriÃ³w. Tak prezentuje siÄ™ widok "scraping" z formularzem przed wyszukaniem wskazanego elementu:
![](https://github.com/Reszke97/aplikacje-internetowe-Reszke-185ic/blob/master/lab5/zrzuty/2.PNG)

## Jako przykÅ‚ad do wyÅ›wietlenia szukanego elementu posÅ‚uÅ¼yÅ‚em siÄ™ stronÄ… https://zacniewski.gitlab.io/ .

## Tak przedstawiajÄ… siÄ™ dane ktÃ³re podaÅ‚em w formularzu:
![](https://github.com/Reszke97/aplikacje-internetowe-Reszke-185ic/blob/master/lab5/zrzuty/3.PNG)

## Po wykonaniu przeszukiwania tak przedstawia siÄ™ widok dla tagu ```p```:
![](https://github.com/Reszke97/aplikacje-internetowe-Reszke-185ic/blob/master/lab5/zrzuty/4.PNG)


## Tak przedstawia siÄ™ funkcja w widokach sÅ‚uÅ¼Ä…ca do pobierania danych:
```python
def scraping (request):
    if request.method == "POST":
        
        websiteLink = request.POST.get('web_link', None)
        element = request.POST.get('element', None)
        url = websiteLink
        source=requests.get(url).text

        allElements = []
        
        soup = BeautifulSoup(source, "html.parser")

        items = soup.find_all(element)
        amount = len(items)    

        for i in items:
            # Szukanie klasy
            findClass = i.get('class')
            if findClass is None:
                findClass = "no matches" 
            
            # Szukanie id
            findId = i.get('id')
            findId = findId.strip() if findId is not None else "no matches"

            # Szukanie article
            findArticle = i.get('article')
            findArticle = findArticle.strip() if findArticle is not None else "no matches"

            # Szukanie tekstu
            getText = i.text
            getText = getText.strip() if getText is not None else "no matches"

            # Szukanie spanÃ³w
            findSpan = i.get('span')
            findSpan = findSpan.strip() if findSpan is not None else "no matches"

            # Szukanie linkÃ³w
            findHref = i.get('href')
            findHref = findHref.strip() if findHref is not None else "no matches"

            allElements.append({"findId": findId, "findClass": findClass, "findArticle": findArticle, "getText": getText, 'findHref':findHref, 'findSpan': findSpan})
        return render(request, 'scrapping/scraping.html', {'allElements':allElements, 'amount': amount, 'websiteLink': websiteLink, 'element':element})
    return render(request, 'scrapping/scraping.html')
```

## Gdy wyÅ›lemy formularz do fuknckji trafia adres url oraz inorfmacja jakiego elementu funkcja ma szukaÄ‡ pod danym adresem url. DziaÅ‚a to w taki sposÃ³b, Å¼e po pobraniu wskazanego elementu poszukiwane sÄ… w nim tagi takie jak:

- span
- article

## Oraz atrybuty :
- id
- class
- href

## Dodatkowo za pomocÄ… funckji .text zwracany jest tekst z danego tagu a w przypadku gdy jest to np. div to jako tekst wyÅ›wietlone zostanie rÃ³wnieÅ¼ wyÅ›wietlone wszystko co jest wewnÄ…trz diva (tzn. np **span** ktÃ³ry posiada jakiÅ› tekst i jest wewnÄ…trz tego diva i wszystkie inne elementy posiadajÄ…ce tekst wchodzÄ…ce w skÅ‚ad tego diva) a nastÄ™pnie za pomocÄ… metody strip() pozbywamy siÄ™ zbÄ™dnych spacji. JeÅ›li jednak ktÃ³ryÅ› z tagÃ³w bÄ…dÅº atrybutÃ³w byÅ‚ pusty czyli po prostu wewnÄ…trz elementu takowy nie wystÄ™puje to zostanie do niego wpisany string "no matches" .

## Tak przedstawia siÄ™ przykÅ‚ad dla elemntu "Div":
![](https://github.com/Reszke97/aplikacje-internetowe-Reszke-185ic/blob/master/lab5/zrzuty/5.PNG)

## Jak moÅ¼na zauwaÅ¼yÄ‡ na rysnku powyÅ¼ej zostaÅ‚o znalezione aÅ¼ 28 elementÃ³w **div** a wewnÄ…trz nich wyÅ›wietlone inne zdefiniowane wczeÅ›niej w funkcji elementy.

## Kodu z szablonu "scraping"(JeÅ›li link istnieje to zostajey on dodany do tagu ``a`` wraz z jego atrybutem ``href``):

```python
{% extends 'scrapping/base.html' %}
{% block content %}
    <form method="POST" class="w40">
        {% csrf_token %}
        <div class="form-group">
            <label>Podaj url</label>
            <input type="text" name="web_link" class="form-control" required placeholder="Url">
        </div>
        <div class="form-group">
            <label for="exampleInputPassword1">Podaj szukany element</label>
            <input type="text" class="form-control" name="element" required placeholder="element">
        </div>
        <button type="submit" class="btn btn-primary">Submit</button>
    </form>

    <ul style="font-family: 'Roboto', sans-serif;font-size:130%;font-weight:800;">
        <li>Przeszukana strona to : <a href = "{{websiteLink}}">{{websiteLink}}</a></li>
        <li>Poszukiwany element to : <span class="text-success">{{element}}</span></li>
        <li>Liczba znalezionych elemntÃ³w : {{amount}}</li>
    </ul>
    <hr></hr>

    {% for elements in allElements %}
        <ul style="padding:2rem">
            <li>klasa--------->{{elements.findClass}}</li>
            <li>id--------->{{elements.findId}}</li>
            <li>span--------->{{elements.findSpan}}</li>
            <li>text--------->{{elements.getText}}</li>
            {% if elements.findHref == 'no matches' %}
                <li>link--------->{{elements.findHref}}<li>
            {% else %}
                <li><a href="{{elements.findHref}}">link--------->{{elements.findHref}}</a></li>
            {% endif %}
        </ul>
    {% endfor %}
{% endblock %}
```

## Na koniec do zastosowania XPath i xmlx stworzyÅ‚em strone z widokiem o nazwie "xpath" w ktÃ³rym zastosowaÅ‚em obie metody. PrzykÅ‚ad z pobraniem elementu za pomocÄ… klasy(przykÅ‚ad nr2) i przykÅ‚ad z pobraniem scieÅ¼ki xpath(przykÅ‚ad nr 1) znajdujÄ… siÄ™ na stronie xPath obok siebie zostanÄ… pokazane po krÃ³tkim omÃ³wieniu jak je znaleÅºÄ‡.

## PrzykÅ‚ad nr 2 to pobranie adresu ``url`` a nastÄ™pnie po zbadaniu elementu pobranie scieÅ¼ki xPath

## Proces przedstawia siÄ™ nastÄ™pujÄ…co. Najpierw ze strony https://www.octoparse.com/blog/top-30-free-web-scraping-software pobieram jej adres url. NastÄ™pnie klikam zbadaj i skopiuj xPath:
![](https://github.com/Reszke97/aplikacje-internetowe-Reszke-185ic/blob/master/lab5/zrzuty/6.png)

## NastÄ™pnie do kodu wrzucam scieÅ¼kÄ™ i kod prezentuje siÄ™ nastÄ™pujÄ…co:
```python
def xml(request):
    # Szukanie elementu poprzez xPath
    url = 'https://www.octoparse.com/blog/top-30-free-web-scraping-software'    #<------------- Tutaj wrzucam adres url
    path = '/html/body/div[2]/div[3]/div[1]/div[1]/div[2]/ul' #<----------------------------Tutaj wrzucam scieÅ¼ke xPath
    response = requests.get(url)
    source = html.fromstring(response.content)    
    tree = source.xpath(path)
    lxmlPrzyklad2 = tree[0].text_content()
```

## PrzykÅ‚ad nr 1 to rÃ³wnieÅ¼ pobranie adresu ```url``` a nastÄ™pnie tym razem skopiwanie nazwy klasy.
## Proces przedstawia siÄ™ nastÄ™pujÄ…co. Najpierw ze strony http://zacniewski.gitlab.io/ pobieram jej adres url. NastÄ™pnie przechodzÄ™ do zbadania elementu i kopiuje nazwÄ™ klasy:
![](https://github.com/Reszke97/aplikacje-internetowe-Reszke-185ic/blob/master/lab5/zrzuty/7.png)

## Teraz pozostaje wrzuciÄ‡ ```url``` i nazwÄ™ klasy do naszego kodu:
```python
  # Szukanie elemntu przez nazwÄ™ klasy   
    url = 'http://zacniewski.gitlab.io/'  #<---------------- Tutaj wrzucam adres url
    path = '//*[@class="well"]' #<---------------- Tutaj wrzucam nazwÄ™ klasy
    response = requests.get(url)    
    source = html.fromstring(response.content)    
    tree = source.xpath(path)
    lxmlPrzyklad1 = tree[0].text_content()
```

## Finalnie funkcja w widokach wyglÄ…da nastÄ™pujÄ…co:
```python
def xml(request):
    
     # Szukanie elementu poprzez xPath
    url = 'https://www.octoparse.com/blog/top-30-free-web-scraping-software'    
    path = '/html/body/div[2]/div[3]/div[1]/div[1]/div[2]/ul'
    response = requests.get(url)
    source = html.fromstring(response.content)    
    tree = source.xpath(path)
    lxmlPrzyklad2 = tree[0].text_content()
    
    # Szukanie elemntu przez nazwÄ™ klasy   
    url = 'http://zacniewski.gitlab.io/'  
    path = '//*[@class="well"]'
    response = requests.get(url)    
    source = html.fromstring(response.content)    
    tree = source.xpath(path)
    lxmlPrzyklad1 = tree[0].text_content()

    return render(request, 'scrapping/xpath.html', {'lxml1': lxmlPrzyklad1,'lxml2': lxmlPrzyklad2 })
```
## Tak wyglÄ…da strona z pobranymi elementami:
![](https://github.com/Reszke97/aplikacje-internetowe-Reszke-185ic/blob/master/lab5/zrzuty/8.PNG)

